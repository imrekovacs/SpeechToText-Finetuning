# Whisper Tiny Fine-tuning Configuration
# ========================================

model:
  name: "openai/whisper-tiny"
  language: "en"
  task: "transcribe"

data:
  train_dir: "data"                    # Directory containing audio + transcripts
  audio_format: "wav"                  # Audio file format (wav, mp3, flac)
  sample_rate: 16000                   # Whisper expects 16kHz
  max_duration_seconds: 30             # Max audio clip length
  test_split: 0.2                      # Fraction of data used for evaluation

# ---------------------------------------------------------------------------
# Fine-tuning strategy
# ---------------------------------------------------------------------------
# strategy:  full | lora | qlora
#   full  = update all model parameters (default, highest quality, most VRAM)
#   lora  = low-rank adapters via PEFT – trains ~0.6% of parameters
#   qlora = 4-bit quantised base + LoRA adapters – lowest VRAM
#
# freeze_encoder: true|false
#   Freeze the audio encoder and only train the decoder.
#   Reduces trainable params by ~50%, prevents catastrophic forgetting of
#   acoustic features. Recommended when your audio domain is similar to the
#   pre-training data (e.g. clean English speech).
#
# gradient_checkpointing: true|false
#   Trade compute for VRAM – recomputes activations during backward pass
#   instead of storing them. Cuts memory ~40% at the cost of ~20% slower
#   training. Helpful when VRAM is tight.

finetuning:
  strategy: "lora"                     # full | lora | qlora
  freeze_encoder: true                 # Freeze encoder weights
  gradient_checkpointing: true         # Reduce VRAM usage

  # LoRA / QLoRA hyper-parameters (ignored when strategy = full)
  lora:
    r: 16                              # Rank of the low-rank matrices
    lora_alpha: 32                     # Scaling factor (alpha / r)
    lora_dropout: 0.05                 # Dropout on adapter weights
    target_modules:                    # Which linear layers to adapt
      - "q_proj"
      - "v_proj"
    bias: "none"                       # none | all | lora_only
    task_type: "SEQ_2_SEQ_LM"         # Whisper is an encoder-decoder model

training:
  output_dir: "output/whisper-tiny-finetuned"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 2
  learning_rate: 1.0e-5
  warmup_steps: 50
  logging_steps: 10
  eval_steps: 50
  save_steps: 100
  save_total_limit: 2
  fp16: true                           # Use mixed precision (requires CUDA)
  dataloader_num_workers: 0            # Set to 0 on Windows
  push_to_hub: false
  report_to: "none"

evaluation:
  metric: "wer"                        # Word Error Rate
